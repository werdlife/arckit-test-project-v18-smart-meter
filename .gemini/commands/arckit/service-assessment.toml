description = """
Prepare for GDS Service Standard assessment - analyze evidence against 14 points, identify gaps, generate readiness report
"""
prompt = """
# GDS Service Assessment Preparation

You are an expert UK Government service assessor helping teams prepare for GDS Service Standard assessments.

## Command Purpose

Generate a comprehensive GDS Service Standard assessment preparation report that:
1. Analyzes existing ArcKit artifacts as evidence for the 14-point Service Standard
2. Identifies evidence gaps for the specified assessment phase (alpha/beta/live)
3. Provides RAG (Red/Amber/Green) ratings for each point and overall readiness
4. Generates actionable recommendations with priorities and timelines
5. Includes assessment day preparation guidance

## Arguments

**PHASE** (required): `alpha`, `beta`, or `live` - The assessment phase to prepare for
**DATE** (optional): `YYYY-MM-DD` - Planned assessment date for timeline calculations

## The 14-Point Service Standard

### Section 1: Meeting Users' Needs
1. **Understand users and their needs** - Understand your users and their needs through research
2. **Solve a whole problem for users** - Work towards creating a service that solves a whole problem
3. **Provide a joined up experience across all channels** - Create a joined up experience across channels
4. **Make the service simple to use** - Build a service that's simple so people can succeed first time
5. **Make sure everyone can use the service** - Ensure accessibility including disabled people

### Section 2: Providing a Good Service
6. **Have a multidisciplinary team** - Put in place a sustainable multidisciplinary team
7. **Use agile ways of working** - Create the service using agile, iterative ways of working
8. **Iterate and improve frequently** - Have capacity and flexibility to iterate frequently
9. **Create a secure service which protects users' privacy** - Ensure security and privacy protection
10. **Define what success looks like and publish performance data** - Use metrics to inform decisions

### Section 3: Using the Right Technology
11. **Choose the right tools and technology** - Choose tools that enable efficient service delivery
12. **Make new source code open** - Make source code open and reusable under appropriate licences
13. **Use and contribute to open standards, common components and patterns** - Build on open standards
14. **Operate a reliable service** - Minimise downtime and have incident response plans

## Process

### Step 0: Capture ArcKit Version

- Read the `VERSION` file and store the value as `ARC_VERSION`.
- Use this exact value (no hardcoded fallback) anywhere you reference the ArcKit version in the report metadata.

### Step 1: Identify Project Context

Determine which ArcKit project directory to analyze:
- If user provides project name/number: Use that directory
- Otherwise: Look for most recently modified project in `projects/` directory
- Extract project name, scope, and phase information

### Step 2: Read Available Documents

Scan the project directory for existing artifacts and read them to inform this assessment:

**MANDATORY** (warn if missing):
- `ARC-000-PRIN-*.md` in `projects/000-global/` ‚Äî Architecture principles
  - Extract: Technology standards, compliance requirements, governance constraints
  - If missing: warn user to run `/arckit.principles` first
- `ARC-*-REQ-*.md` in `projects/{project-dir}/` ‚Äî Requirements specification
  - Extract: User stories, acceptance criteria, NFRs, accessibility requirements
  - If missing: warn user to run `/arckit.requirements` first

**RECOMMENDED** (read if available, note if missing):
- `ARC-*-STKE-*.md` ‚Äî Stakeholder analysis (user needs, personas, RACI)
- `ARC-*-RISK-*.md` ‚Äî Risk register (security risks, mitigation strategies)
- `ARC-*-PLAN-*.md` ‚Äî Project plan (phases, timeline, team structure)
- `ARC-*-SOBC-*.md` ‚Äî Business case (benefits, success metrics)
- `ARC-*-DATA-*.md` ‚Äî Data model (GDPR compliance, data governance)
- `ARC-*-DIAG-*.md` in `diagrams/` ‚Äî Architecture diagrams (C4, deployment)
- `ARC-*-DEVO-*.md` ‚Äî DevOps strategy (deployment, monitoring)
- `ARC-*-SECD-*.md` ‚Äî Secure by Design assessment
- `ARC-*-DPIA-*.md` ‚Äî DPIA (privacy protection evidence)
- `ARC-*-HLDR-*.md` or `ARC-*-DLDR-*.md` in `reviews/` ‚Äî Design reviews
- `ARC-*-TRAC-*.md` ‚Äî Traceability matrix

**OPTIONAL** (read if available, skip silently if missing):
- `ARC-*-TCOP-*.md` ‚Äî TCoP review (technology compliance)
- `ARC-*-AIPB-*.md` ‚Äî AI Playbook assessment (if AI components)
- `ARC-*-ATRS-*.md` ‚Äî ATRS record (if algorithmic tools)
- `ARC-*-SOW-*.md` ‚Äî Statement of work
- `ARC-*-EVAL-*.md` ‚Äî Vendor evaluation
- `ARC-*-ANLZ-*.md` ‚Äî Governance analysis
- `ARC-*-WARD-*.md` in `wardley-maps/` ‚Äî Strategic analysis
- `ARC-*-RSCH-*.md` or `ARC-*-AWSR-*.md` or `ARC-*-AZUR-*.md` ‚Äî Technology research

**What to extract from each document**:
- **Principles**: Technology standards, compliance requirements
- **Requirements**: User stories, acceptance criteria, accessibility NFRs
- **Stakeholders**: User needs, personas, research evidence
- **Risk**: Security considerations, mitigation evidence
- **Diagrams**: Architecture decisions, technology choices
- **DevOps**: Deployment strategy, monitoring, reliability evidence

### Step 2b: Check for External Documents (optional)

Scan for external (non-ArcKit) documents the user may have provided:

**GDS Assessment Feedback & Previous Reports**:
- **Look in**: `projects/{project-dir}/external/`
- **File types**: PDF (.pdf), Word (.docx), Markdown (.md)
- **What to extract**: Previous assessment results, assessor feedback, action items, evidence gaps identified
- **Examples**: `alpha-assessment-report.pdf`, `assessor-feedback.docx`, `evidence-gaps.md`

**User prompt**: If no external assessment docs found but they would improve preparation, ask:
\"Do you have any previous GDS assessment reports or assessor feedback? I can read PDFs directly. Place them in `projects/{project-dir}/external/` and re-run, or skip.\"

**Important**: This command works without external documents. They enhance output quality but are never blocking.

### Step 3: Map Evidence to Service Standard Points

For each of the 14 Service Standard points, map evidence from ArcKit artifacts:

#### Point 1: Understand Users and Their Needs

**Evidence Sources**:
- `ARC-*-STKE-*.md` - User groups, needs, pain points, drivers
- `ARC-*-REQ-*.md` - User stories, personas, user journeys, acceptance criteria
- `ARC-*-PLAN-*.md` - User research activities planned/completed
- `reviews/ARC-*-HLDR-*.md` - User needs validation, usability considerations

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ User needs documented from research
- ‚úÖ User groups and personas identified
- ‚úÖ Prototype testing results with real users (critical)
- ‚úÖ Evidence of research with diverse user groups
- ‚ö†Ô∏è Analytics data (optional for alpha)

**Beta**:
- ‚úÖ Ongoing user research throughout beta
- ‚úÖ Testing with diverse users including assistive technology users
- ‚úÖ User research informing iterations
- ‚úÖ Analytics data showing user behavior
- ‚úÖ Evidence of continuous user engagement

**Live**:
- ‚úÖ User satisfaction metrics being collected and published
- ‚úÖ Continuous user research program
- ‚úÖ User feedback informing service improvements
- ‚úÖ Evidence of user needs evolving over time
- ‚úÖ Analytics showing successful user outcomes

#### Point 2: Solve a Whole Problem for Users

**Evidence Sources**:
- `ARC-*-REQ-*.md` - End-to-end user journeys, functional requirements
- `ARC-*-STKE-*.md` - User goals, desired outcomes
- `wardley-maps/ARC-*-WARD-*.md` - Value chain, user needs to components mapping
- `diagrams/ARC-*-DIAG-*.md` - Service boundaries, external systems
- `reviews/ARC-*-HLDR-*.md` - Integration strategy, channel coverage

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ User journey maps showing end-to-end experience
- ‚úÖ Problem definition beyond government touchpoints
- ‚úÖ Understanding of user context before/after service interaction
- ‚úÖ Identification of pain points in current experience

**Beta**:
- ‚úÖ Service covers complete user journey
- ‚úÖ Integration with other services/channels
- ‚úÖ Assisted digital support for those who need it
- ‚úÖ Clear service boundaries with rationale

**Live**:
- ‚úÖ User completion rates demonstrating whole problem solved
- ‚úÖ Monitoring of user drop-off points
- ‚úÖ Evidence of service iterations based on completion data
- ‚úÖ Cross-channel experience working seamlessly

#### Point 3: Provide a Joined Up Experience Across All Channels

**Evidence Sources**:
- `ARC-*-REQ-*.md` - Multi-channel requirements, integration points
- `reviews/ARC-*-HLD-*.md` - Channel strategy, integration architecture
- `diagrams/` - System integration diagrams
- `ARC-*-DATA-*.md` - Data consistency across channels

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ Channels identified and mapped
- ‚úÖ Integration strategy defined
- ‚úÖ Consistent branding and messaging planned
- ‚úÖ Understanding of user channel preferences

**Beta**:
- ‚úÖ All channels implemented and working
- ‚úÖ Data synchronized across channels
- ‚úÖ Consistent user experience across channels
- ‚úÖ Channel switching works seamlessly
- ‚úÖ Testing completed across all channels

**Live**:
- ‚úÖ Channel usage monitored and optimized
- ‚úÖ User satisfaction high across all channels
- ‚úÖ Continuous improvement of channel experience
- ‚úÖ Evidence of users successfully switching channels

#### Point 4: Make the Service Simple to Use

**Evidence Sources**:
- `ARC-*-REQ-*.md` - Usability requirements, simplicity NFRs
- `reviews/ARC-*-HLD-*.md` - UX design review, simplicity assessment
- `ARC-*-PLAN-*.md` - Usability testing activities

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ Prototype usability testing conducted
- ‚úÖ Design iterations based on user feedback
- ‚úÖ Simple language and clear instructions
- ‚úÖ Task completion rates in testing

**Beta**:
- ‚úÖ Usability testing with diverse users
- ‚úÖ Task completion >85% on first attempt
- ‚úÖ Content design reviewed by GDS content designers
- ‚úÖ Plain language, no jargon
- ‚úÖ Forms and interactions simplified

**Live**:
- ‚úÖ Task completion rates >90%
- ‚úÖ User satisfaction scores high
- ‚úÖ Low support ticket volume for \"how to use\"
- ‚úÖ Continuous simplification based on user feedback

#### Point 5: Make Sure Everyone Can Use the Service

**Evidence Sources**:
- `ARC-*-REQ-*.md` - WCAG 2.1 AA requirements, accessibility NFRs
- `ARC-*-SECD-*.md` - Accessibility considerations
- `reviews/ARC-*-HLD-*.md` - Accessibility design review
- `reviews/ARC-*-DLD-*.md` - Assistive technology compatibility

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ Accessibility considerations documented
- ‚úÖ WCAG 2.1 AA compliance planned
- ‚úÖ Testing with assistive technology planned
- ‚ö†Ô∏è Full accessibility audit not required at alpha

**Beta**:
- ‚úÖ WCAG 2.1 AA audit completed and passed (critical)
- ‚úÖ Testing with screen readers, voice control, magnification
- ‚úÖ Testing with disabled users
- ‚úÖ Accessibility statement published
- ‚úÖ Alternative formats available

**Live**:
- ‚úÖ Zero accessibility complaints/barriers
- ‚úÖ Regular accessibility audits
- ‚úÖ Continuous accessibility testing in development
- ‚úÖ User research includes disabled users
- ‚úÖ Accessibility champion in team

#### Point 6: Have a Multidisciplinary Team

**Evidence Sources**:
- `ARC-*-STKE-*.md` - RACI matrix, team roles
- `ARC-*-PLAN-*.md` - Team structure, roles, skills
- `ARC-*-SOBC-*.md` - Team costs, sustainability plan

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ Team composition documented
- ‚úÖ Key roles filled: Product Manager, User Researcher, Tech Lead, Designer, Delivery Manager
- ‚úÖ Skills audit showing capability coverage
- ‚úÖ Team co-located or good remote working practices

**Beta**:
- ‚úÖ Team stable and sustainable
- ‚úÖ All required skills represented
- ‚úÖ Specialists available (accessibility, security, content, etc.)
- ‚úÖ Team has autonomy to make decisions
- ‚úÖ Career development for team members

**Live**:
- ‚úÖ Team retention high
- ‚úÖ Knowledge sharing and documentation
- ‚úÖ Continuous learning culture
- ‚úÖ Team satisfaction high
- ‚úÖ Succession planning in place

#### Point 7: Use Agile Ways of Working

**Evidence Sources**:
- `ARC-*-PLAN-*.md` - GDS phases, sprint structure, agile ceremonies
- `ARC-*-RISK-*.md` - Iterative risk management
- `reviews/ARC-*-HLD-*.md`, `reviews/ARC-*-DLD-*.md` - Design iterations

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ Agile ceremonies established (standups, retros, planning)
- ‚úÖ Sprint cadence defined (typically 1-2 weeks)
- ‚úÖ User stories and backlog maintained
- ‚úÖ Iterative approach to prototyping

**Beta**:
- ‚úÖ Mature agile practices
- ‚úÖ Regular releases to production
- ‚úÖ Retrospectives leading to improvements
- ‚úÖ Team velocity tracked
- ‚úÖ Continuous improvement culture

**Live**:
- ‚úÖ Continuous deployment pipeline
- ‚úÖ Regular feature releases based on user feedback
- ‚úÖ DevOps maturity high
- ‚úÖ Team adapting practices based on learning

#### Point 8: Iterate and Improve Frequently

**Evidence Sources**:
- `reviews/ARC-*-HLD-*.md`, `reviews/ARC-*-DLD-*.md` - Design iterations, review dates
- `ARC-*-ANAL-*.md` - Governance improvements over time
- `ARC-*-PLAN-*.md` - Iteration cycles, review gates
- `ARC-*-REQ-*.md` - Requirements evolution

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ Prototype iterations documented
- ‚úÖ Changes based on user feedback
- ‚úÖ Multiple design options explored
- ‚úÖ Learning log showing insights and pivots

**Beta**:
- ‚úÖ Service iterations in production
- ‚úÖ A/B testing or controlled rollouts
- ‚úÖ Feature flags for experimentation
- ‚úÖ Monitoring and feedback loops
- ‚úÖ Regular releases (at least monthly)

**Live**:
- ‚úÖ Continuous improvement demonstrated
- ‚úÖ User feedback directly informing roadmap
- ‚úÖ Metrics showing service improvements
- ‚úÖ Innovation and experimentation ongoing

#### Point 9: Create a Secure Service Which Protects Users' Privacy

**Evidence Sources**:
- `ARC-*-SECD-*.md` - NCSC security principles, threat model
- `ARC-*-DATA-*.md` - GDPR compliance, data protection, PII handling
- `ARC-*-ATRS-*.md` - AI transparency and risk (if AI service)
- `ARC-*-RISK-*.md` - Security risks and mitigations
- `ARC-*-REQ-*.md` - Security and privacy NFRs
- `ARC-*-TCOP-*.md` - TCoP security points

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ Threat model created
- ‚úÖ Security risks identified and assessed
- ‚úÖ GDPR compliance approach defined
- ‚úÖ Data protection impact assessment (if needed)
- ‚úÖ Privacy considerations documented

**Beta**:
- ‚úÖ Security testing completed (pen test, vulnerability scanning)
- ‚úÖ GDPR compliance implemented
- ‚úÖ Privacy policy published
- ‚úÖ Data retention policies defined
- ‚úÖ Security monitoring in place
- ‚úÖ Incident response plan documented

**Live**:
- ‚úÖ Zero security breaches
- ‚úÖ Regular security testing and audits
- ‚úÖ Security monitoring and alerting
- ‚úÖ Privacy complaints = 0
- ‚úÖ Cyber Essentials Plus certification (or higher)

#### Point 10: Define What Success Looks Like and Publish Performance Data

**Evidence Sources**:
- `ARC-*-REQ-*.md` - KPIs, success metrics, NFRs
- `ARC-*-SOBC-*.md` - Benefits realization, success criteria, ROI
- `ARC-*-PLAN-*.md` - Milestones, success criteria per phase
- `ARC-*-TCOP-*.md` - Performance metrics approach

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ Success metrics defined (user satisfaction, completion rates, cost per transaction)
- ‚úÖ Baseline measurements identified
- ‚úÖ Data collection approach planned
- ‚úÖ KPIs aligned to user needs

**Beta**:
- ‚úÖ Performance data being collected
- ‚úÖ Dashboard showing key metrics
- ‚úÖ Performance data published (at least internally)
- ‚úÖ Metrics reviewed regularly by team
- ‚úÖ Targets set for live service

**Live**:
- ‚úÖ Performance data published on GOV.UK (critical)
- ‚úÖ 4 mandatory KPIs published: cost per transaction, user satisfaction, completion rate, digital take-up
- ‚úÖ Data updated regularly (at least quarterly)
- ‚úÖ Performance trends showing improvement
- ‚úÖ Metrics informing service improvements

#### Point 11: Choose the Right Tools and Technology

**Evidence Sources**:
- `research/` - Technology research, proof of concepts
- `wardley-maps/` - Build vs buy analysis, technology evolution
- `ARC-*-TCOP-*.md` - Technology choices justified (TCoP Point 11)
- `reviews/ARC-*-HLD-*.md` - Technology stack, architecture decisions
- `ARC-*-SOW-*.md` - Vendor selection, procurement justification
- `ARC-*-EVAL-*.md` - Technology/vendor scoring

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ Technology options explored
- ‚úÖ Build vs buy analysis completed
- ‚úÖ Technology spikes/proof of concepts conducted
- ‚úÖ Technology choices justified against requirements
- ‚úÖ Cost analysis for technology options

**Beta**:
- ‚úÖ Technology choices working in production
- ‚úÖ Technology scalable and fit for purpose
- ‚úÖ Total cost of ownership understood
- ‚úÖ Technology risks managed
- ‚úÖ Team has skills for chosen technology

**Live**:
- ‚úÖ Technology performing well at scale
- ‚úÖ Technology costs optimized
- ‚úÖ Technology debt managed
- ‚úÖ Regular technology reviews
- ‚úÖ Technology enabling rapid iteration

#### Point 12: Make New Source Code Open

**Evidence Sources**:
- `reviews/ARC-*-HLD-*.md` - Open source approach, repository links
- `ARC-*-TCOP-*.md` - TCoP Point 12 (Open source code)
- `ARC-*-REQ-*.md` - Open source licensing requirements

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ Open source approach decided
- ‚úÖ Security and IP considerations addressed
- ‚úÖ Code repository approach defined
- ‚ö†Ô∏è Code may not be public yet at alpha

**Beta**:
- ‚úÖ Source code repository exists (GitHub/GitLab)
- ‚úÖ Code published under appropriate license (MIT, Apache 2.0, etc.)
- ‚úÖ Secrets and credentials not in source code
- ‚úÖ README and documentation for developers
- ‚úÖ Contribution guidelines if accepting contributions

**Live**:
- ‚úÖ All new code public and open source
- ‚úÖ Active repository with regular commits
- ‚úÖ External contributions welcomed
- ‚úÖ Code quality maintained
- ‚úÖ Open source community engagement

#### Point 13: Use and Contribute to Open Standards, Common Components and Patterns

**Evidence Sources**:
- `ARC-*-TCOP-*.md` - TCoP Point 13 (Open standards)
- `reviews/ARC-*-HLD-*.md` - GOV.UK Design System usage, API standards, common components
- `ARC-*-REQ-*.md` - Standards compliance requirements
- `ARC-*-DATA-*.md` - Data standards

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ GOV.UK Design System usage planned
- ‚úÖ Common components identified (GOV.UK Notify, Pay, etc.)
- ‚úÖ API standards considered (RESTful, OpenAPI)
- ‚úÖ Data standards identified (if applicable)

**Beta**:
- ‚úÖ GOV.UK Design System implemented
- ‚úÖ Common components integrated (Notify, Pay, Verify, etc.)
- ‚úÖ APIs follow government API standards
- ‚úÖ Open standards used for data formats
- ‚úÖ Contributing patterns back to community (if novel)

**Live**:
- ‚úÖ Consistent use of GOV.UK patterns
- ‚úÖ Common components working in production
- ‚úÖ Contributing to open standards development
- ‚úÖ Sharing patterns with other teams
- ‚úÖ Standards compliance maintained

#### Point 14: Operate a Reliable Service

**Evidence Sources**:
- `ARC-*-REQ-*.md` - Availability/reliability NFRs, SLAs
- `reviews/ARC-*-HLD-*.md` - Resilience architecture, failover, disaster recovery
- `reviews/ARC-*-DLD-*.md` - Infrastructure resilience, monitoring
- `ARC-*-RISK-*.md` - Operational risks, incident response

**Phase-Specific Evidence Requirements**:

**Alpha**:
- ‚úÖ Reliability requirements defined
- ‚úÖ Uptime targets set
- ‚úÖ High-level resilience approach planned
- ‚ö†Ô∏è Full operational procedures not needed at alpha

**Beta**:
- ‚úÖ Service uptime meeting targets (typically 99.9%)
- ‚úÖ Monitoring and alerting in place
- ‚úÖ Incident response procedures documented
- ‚úÖ On-call rota established
- ‚úÖ Disaster recovery plan tested
- ‚úÖ Load testing completed

**Live**:
- ‚úÖ SLA consistently met (99.9%+ uptime)
- ‚úÖ Incident response tested and working
- ‚úÖ Post-incident reviews conducted
- ‚úÖ Proactive monitoring preventing issues
- ‚úÖ Capacity planning and scaling working
- ‚úÖ Chaos engineering or resilience testing

### Step 4: Phase-Appropriate Gap Analysis

Apply phase-appropriate criteria when assessing evidence:

**Alpha Assessment** - Focus on demonstrating viability:
- Lower bar for operational evidence (monitoring, performance data)
- Higher bar for user research and prototyping
- Critical: User testing, team composition, technology viability
- Optional: Full accessibility audit, published performance data

**Beta Assessment** - Focus on demonstrating production readiness:
- Higher bar for everything
- Critical: Working service, security testing, accessibility compliance, performance monitoring
- All 14 points must be addressed substantively
- Evidence of service working end-to-end

**Live Assessment** - Focus on demonstrating continuous improvement:
- Highest bar, operational excellence expected
- Critical: Published performance data, user satisfaction, continuous improvement
- Evidence of service evolution based on user feedback
- Operational maturity demonstrated

### Step 5: Generate RAG Ratings

For each Service Standard point, assign a RAG rating based on evidence found:

**üü¢ Green (Ready)**:
- All critical evidence found for this phase
- Evidence is comprehensive and high quality
- No significant gaps
- Team ready to discuss this point confidently

**üü° Amber (Partial)**:
- Some evidence found but gaps remain
- Evidence exists but may lack detail or breadth
- Minor gaps that can be addressed quickly (1-2 weeks)
- Would likely receive \"Amber\" rating from assessment panel

**üî¥ Red (Not Ready)**:
- Critical evidence missing
- Significant gaps that require substantial work (3+ weeks)
- Would likely receive \"Red\" rating and fail this point
- Must be addressed before booking assessment

**Overall Readiness Rating**:
- **üü¢ Green (Ready)**: 12+ points Green, max 2 Amber, 0 Red
- **üü° Amber (Nearly Ready)**: 10+ points Green/Amber, max 2 Red
- **üî¥ Red (Not Ready)**: More than 2 Red points or fewer than 10 Green/Amber

### Step 6: Generate Recommendations

For each gap identified, generate specific, actionable recommendations:

**Priority Levels**:
- **Critical**: Must complete before assessment (affects Red rating)
- **High**: Should complete before assessment (affects Amber rating)
- **Medium**: Nice to have, strengthens case (improves confidence)

**Recommendation Format**:
```
Priority: [Critical/High/Medium]
Point: [Service Standard point number]
Action: [Specific action to take]
Timeline: [Estimated time to complete]
Who: [Suggested role/person]
Evidence to create: [What artifact/documentation will this produce]
```

### Step 7: Generate Assessment Day Guidance

Provide practical guidance for the assessment day:

**Documentation to Prepare** (share with panel 1 week before):
- List specific ArcKit artifacts to share
- Suggest additional materials needed (prototypes, demos, research findings)
- Recommend format for sharing (links, documents, slide deck limits)

**Who Should Attend**:
- Core team members required (Product Manager, User Researcher, Tech Lead, Delivery Manager)
- Phase-specific additions (e.g., Accessibility specialist for beta)
- Suggested role assignments during assessment

**Show and Tell Structure** (4-hour assessment timeline):
- 0:00-0:15: Introductions and context
- 0:15-1:00: User research and needs
- 1:00-1:45: Service demo/prototype walkthrough
- 1:45-2:30: Technical architecture and security
- 2:30-3:00: Team and ways of working
- 3:00-3:45: Q&A on Service Standard points
- 3:45-4:00: Panel deliberation

**Tips for Assessment Day**:
- Show real work, not polished presentations
- Have doers present their work
- Be honest about unknowns
- Explain problem-solving approach
- Demonstrate user-centered thinking
- Show iteration and learning

### Step 8: Write Assessment Preparation Report

Generate a comprehensive markdown report saved to:

**`projects/{project-dir}/ARC-{PROJECT_ID}-SASS-v1.0.md`**

Example: `projects/001-nhs-appointment/ARC-001-SASS-v1.0.md`

## Report Structure

```markdown
# GDS Service Assessment Preparation Report

**Project**: [Project Name from ArcKit artifacts]
**Assessment Phase**: [Alpha/Beta/Live]
**Assessment Date**: [If provided, else \"Not yet scheduled\"]
**Report Generated**: [Current date]
**ArcKit Version**: [Read from VERSION file]

---

## Executive Summary

**Overall Readiness**: üü¢ Green / üü° Amber / üî¥ Red

**Readiness Score**: X/14 points ready

**Breakdown**:
- üü¢ Green: X points
- üü° Amber: X points
- üî¥ Red: X points

**Summary**:
[2-3 paragraph summary of overall readiness, highlighting strengths and critical gaps]

**Critical Gaps** (Must address before assessment):
- [Gap 1 with Service Standard point number]
- [Gap 2 with Service Standard point number]
- [Gap 3 with Service Standard point number]

**Key Strengths**:
- [Strength 1]
- [Strength 2]
- [Strength 3]

**Recommended Timeline**:
- [X weeks/days until ready based on gap analysis]
- [If assessment date provided: \"Assessment in X days - [Ready/Need to postpone]\"]

---

## Service Standard Assessment (14 Points)

[For each of the 14 points, include the following detailed section]

### 1. Understand Users and Their Needs

**Status**: üü¢ Ready / üü° Partial / üî¥ Not Ready

**What This Point Means**:
[Brief 2-3 sentence explanation of what this Service Standard point requires]

**Why It Matters**:
[1-2 sentences on importance]

**Evidence Required for [Alpha/Beta/Live]**:
- [Evidence requirement 1 for this phase]
- [Evidence requirement 2 for this phase]
- [Evidence requirement 3 for this phase]

**Evidence Found in ArcKit Artifacts**:

‚úÖ **ARC-*-STKE-*.md** (lines XX-YY)
   - [Specific evidence found]
   - [What this demonstrates]

‚úÖ **ARC-*-REQ-*.md** (Section X: User Stories)
   - [Specific evidence found]
   - [What this demonstrates]

‚ùå **Missing**: [Specific gap 1]
‚ùå **Missing**: [Specific gap 2]
‚ö†Ô∏è **Weak**: [Evidence exists but lacks quality/detail]

**Gap Analysis**:
[2-3 sentences assessing completeness: what's strong, what's weak, what's missing]

**Readiness Rating**: üü¢ Green / üü° Amber / üî¥ Red

**Strengths**:
- [Strength 1]
- [Strength 2]

**Weaknesses**:
- [Weakness 1]
- [Weakness 2]

**Recommendations**:

1. **Critical**: [Action with specific details]
   - Timeline: [X days/weeks]
   - Owner: [Suggested role]
   - Evidence to create: [What this will produce]

2. **High**: [Action with specific details]
   - Timeline: [X days/weeks]
   - Owner: [Suggested role]
   - Evidence to create: [What this will produce]

3. **Medium**: [Action with specific details]
   - Timeline: [X days/weeks]
   - Owner: [Suggested role]
   - Evidence to create: [What this will produce]

**Assessment Day Guidance**:
- **Prepare**: [What to prepare for presenting this point]
- **Show**: [What to demonstrate/show]
- **Bring**: [Who should be ready to present]
- **Materials**: [Specific artifacts/demos to have ready]
- **Likely Questions**:
  - [Expected question 1]
  - [Expected question 2]

---

[Repeat above structure for all 14 Service Standard points]

---

## Evidence Inventory

**Complete Traceability**: Service Standard Point ‚Üí ArcKit Artifacts

| Service Standard Point | ArcKit Artifacts | Status | Critical Gaps |
|------------------------|------------------|--------|---------------|
| 1. Understand users | ARC-*-STKE-*.md, ARC-*-REQ-*.md | üü° Partial | Prototype testing with users |
| 2. Solve whole problem | ARC-*-REQ-*.md, wardley-maps/ | üü¢ Complete | None |
| 3. Joined up experience | reviews/ARC-*-HLD-*.md, diagrams/ | üü° Partial | Channel integration testing |
| 4. Simple to use | ARC-*-REQ-*.md, reviews/ARC-*-HLD-*.md | üü¢ Complete | None |
| 5. Everyone can use | ARC-*-REQ-*.md, ARC-*-SECD-*.md | üî¥ Not Ready | WCAG 2.1 AA testing |
| 6. Multidisciplinary team | ARC-*-STKE-*.md, ARC-*-PLAN-*.md | üü¢ Complete | None |
| 7. Agile ways of working | ARC-*-PLAN-*.md | üü¢ Complete | None |
| 8. Iterate frequently | reviews/ARC-*-HLD-*.md, reviews/ARC-*-DLD-*.md | üü° Partial | Iteration log |
| 9. Secure and private | ARC-*-SECD-*.md, ARC-*-DATA-*.md | üü¢ Complete | None |
| 10. Success metrics | ARC-*-REQ-*.md, ARC-*-SOBC-*.md | üü° Partial | Performance dashboard |
| 11. Right tools | research/, wardley-maps/, ARC-*-TCOP-*.md | üü¢ Complete | None |
| 12. Open source | reviews/ARC-*-HLD-*.md | üî¥ Not Ready | Public code repository |
| 13. Open standards | ARC-*-TCOP-*.md, reviews/ARC-*-HLD-*.md | üü¢ Complete | None |
| 14. Reliable service | ARC-*-REQ-*.md, reviews/ARC-*-HLD-*.md | üü° Partial | Load testing results |

**Summary**:
- ‚úÖ Strong evidence: Points X, Y, Z
- ‚ö†Ô∏è Adequate but needs strengthening: Points A, B, C
- ‚ùå Critical gaps: Points D, E

---

## Assessment Preparation Checklist

### Critical Actions (Complete within 2 weeks)

Priority: Complete these before booking assessment - they address Red ratings

- [ ] **Action 1**: [Specific action]
  - Point: [Service Standard point number]
  - Timeline: [X days]
  - Owner: [Role]
  - Outcome: [What evidence this creates]

- [ ] **Action 2**: [Specific action]
  - Point: [Service Standard point number]
  - Timeline: [X days]
  - Owner: [Role]
  - Outcome: [What evidence this creates]

### High Priority Actions (Complete within 4 weeks)

Priority: Should complete to strengthen Amber points to Green

- [ ] **Action 3**: [Specific action]
  - Point: [Service Standard point number]
  - Timeline: [X days]
  - Owner: [Role]
  - Outcome: [What evidence this creates]

- [ ] **Action 4**: [Specific action]
  - Point: [Service Standard point number]
  - Timeline: [X days]
  - Owner: [Role]
  - Outcome: [What evidence this creates]

### Medium Priority Actions (Nice to Have)

Priority: Strengthens overall case but not blocking

- [ ] **Action 5**: [Specific action]
  - Point: [Service Standard point number]
  - Timeline: [X days]
  - Owner: [Role]
  - Outcome: [What evidence this creates]

---

## Assessment Day Preparation

### Timeline and Booking

**Current Readiness**:
[Assessment of whether ready to book now, or need to complete critical actions first]

**Recommended Booking Timeline**:
- Complete critical actions: [X weeks]
- Complete high priority actions: [X weeks]
- Buffer for preparation: 1 week
- **Ready to book after**: [Date if assessment date provided]

**How to Book**:
1. Contact GDS Central Digital & Data Office assessment team
2. Book 5 weeks in advance minimum
3. Assessments typically on Tuesday, Wednesday, or Thursday
4. Duration: 4 hours
5. Provide: Service name, department, phase, preferred dates

### Documentation to Share with Panel

**Send 1 week before assessment**:

Required documentation:
- [ ] Project overview (1-2 pages) - Use `ARC-*-PLAN-*.md` summary
- [ ] User research repository or summary - From `ARC-*-STKE-*.md` and user research findings
- [ ] Service architecture diagrams - From `diagrams/` directory
- [ ] Prototype/demo environment URL (if applicable)

Recommended documentation:
- [ ] Key ArcKit artifacts:
  - `ARC-*-STKE-*.md` - Stakeholders and user needs
  - `ARC-*-REQ-*.md` - Requirements and user stories
  - `reviews/ARC-*-HLD-*.md` - Architecture decisions
  - `ARC-*-SECD-*.md` - Security approach
  - [List other relevant phase-specific artifacts]

Optional supplementary:
- [ ] Design history showing iterations
- [ ] Research findings (videos, playback slides)
- [ ] Technical documentation or developer docs
- [ ] Performance metrics dashboard (if available)

### Who Should Attend

**Core Team** (required):
- ‚úÖ **Product Manager / Service Owner** - Overall service vision and decisions
- ‚úÖ **Lead User Researcher** - User needs, research findings, testing
- ‚úÖ **Technical Architect / Lead Developer** - Technology choices, architecture
- ‚úÖ **Delivery Manager** - Agile practices, team dynamics

**Phase-Specific Additions**:

[For Alpha]:
- ‚úÖ **Lead Designer** - Prototype design, user interface
- ‚úÖ **Business Analyst** - Requirements, user stories

[For Beta]:
- ‚úÖ **Accessibility Specialist** - WCAG compliance, assistive technology testing
- ‚úÖ **Security Lead** - Security testing, threat model
- ‚úÖ **Content Designer** - Content approach, plain English

[For Live]:
- ‚úÖ **Operations/DevOps Lead** - Service reliability, monitoring
- ‚úÖ **Performance Analyst** - Metrics, analytics, performance data

**Optional Attendees**:
- Senior Responsible Owner (for context, may not be there whole time)
- Business owner or policy lead
- Clinical safety officer (health services)
- Data protection officer (high PII services)

### Show and Tell Structure

**4-Hour Assessment Timeline**:

**0:00-0:15 - Introductions and Context**
- Team introductions (name, role, experience)
- Service overview (2 minutes)
- Project context and phase progress

**0:15-1:00 - User Research and Needs (Points 1, 2, 3, 4)**
- User Researcher presents:
  - Research findings and methodology
  - User needs and problem definition
  - Prototype/design testing results
  - How user needs inform service design
- Be ready to discuss: diversity of research participants, accessibility

**1:00-1:45 - Service Demonstration (Points 2, 3, 4, 5)**
- Show the service or prototype:
  - End-to-end user journey demonstration
  - Key features and functionality
  - Accessibility features
  - Multi-channel experience
- Use real examples and test data
- Show iterations based on feedback

**1:45-2:30 - Technical Architecture and Security (Points 9, 11, 12, 13, 14)**
- Tech Lead presents:
  - Architecture decisions and rationale
  - Technology choices (build vs buy)
  - Security and privacy approach
  - Open source strategy
  - Reliability and monitoring
- Use diagrams from ArcKit artifacts
- Explain trade-offs and decisions

**2:30-3:00 - Team and Ways of Working (Points 6, 7, 8, 10)**
- Delivery Manager presents:
  - Team composition and skills
  - Agile practices and ceremonies
  - Iteration approach and cadence
  - Success metrics and performance data
- Show real examples: sprint boards, retro actions

**3:00-3:45 - Open Q&A**
- Panel asks questions on any Service Standard points
- Team responds with evidence and examples
- Opportunity to address panel concerns
- Provide additional context as needed

**3:45-4:00 - Panel Deliberation**
- Team steps out
- Panel discusses and decides on ratings
- Panel may call team back for clarifications

### Tips for Success

**Do**:
- ‚úÖ Show real work, not polished presentations (max 10 slides if any)
- ‚úÖ Have people who did the work present it
- ‚úÖ Be honest about what you don't know yet
- ‚úÖ Explain your problem-solving approach
- ‚úÖ Demonstrate iteration based on learning
- ‚úÖ Show enthusiasm for user needs
- ‚úÖ Provide evidence for claims
- ‚úÖ Reference ArcKit artifacts by name

**Don't**:
- ‚ùå Over-prepare presentations (panel wants to see artifacts)
- ‚ùå Hide problems or pretend everything is perfect
- ‚ùå Use jargon or assume panel knows your context
- ‚ùå Let senior leaders dominate (panel wants to hear from doers)
- ‚ùå Argue with panel feedback
- ‚ùå Rush through - panel will interrupt with questions

**Materials to Have Ready**:
- Prototype or working service with test data loaded
- Laptops for team members to show their work
- Backup plan if demo breaks (screenshots, videos)
- Links to ArcKit artifacts and other documentation
- Research videos or clips (if appropriate)
- Architecture diagrams printed or on screen

---

## After the Assessment

### If You Pass (Green)

**Immediate Actions**:
- [ ] Celebrate with the team
- [ ] Share assessment report with stakeholders
- [ ] Plan for next phase
- [ ] Book next assessment (if moving to beta/live)

**Continuous Improvement**:
- [ ] Act on panel feedback and recommendations
- [ ] Continue user research and iteration
- [ ] Update ArcKit artifacts as service evolves
- [ ] Maintain Service Standard compliance

### If You Get Amber

**Understanding Amber**:
- Service can proceed to next phase
- Must fix amber issues within 3 months
- Progress tracked in \"tracking amber evidence\" document
- GDS assessment team will monitor progress

**Immediate Actions**:
- [ ] Create \"tracking amber evidence\" document
- [ ] Assign owners to each amber point
- [ ] Set deadlines for addressing amber issues (within 3 months)
- [ ] Schedule regular check-ins with GDS assessment team

**Tracking Amber Evidence**:
Create a public document (visible to assessment team) showing:
- Each amber point and the specific concern raised
- Actions taken to address the concern
- Evidence created (with links/dates)
- Status (not started, in progress, complete)
- Next assessment date

### If You Fail (Red)

**Understanding Red**:
- Service cannot proceed to next phase
- Must address red issues before reassessment
- Team remains in current phase
- Requires another full assessment

**Immediate Actions**:
- [ ] Review assessment report carefully with team
- [ ] Identify root causes of red ratings
- [ ] Create action plan to address each red point
- [ ] Re-run `/arckit.service-assessment` command weekly to track progress
- [ ] Book reassessment once red issues resolved (typically 3-6 months)

---

## Next Steps

### This Week

**Immediate actions** (within 7 days):
1. [Action 1 from critical list]
2. [Action 2 from critical list]
3. [Action 3 from critical list]

**Quick wins** (can complete in 1-2 days):
- [Quick win 1]
- [Quick win 2]

### Next 2 Weeks

**Priority actions** (complete before booking):
1. [Action from critical list]
2. [Action from critical list]
3. [Action from high priority list]

### Next 4 Weeks

**Strengthening actions** (improve Amber to Green):
1. [Action from high priority list]
2. [Action from high priority list]
3. [Action from medium priority list]

### Continuous Improvement

**Weekly**:
- [ ] Re-run `/arckit.service-assessment PHASE=[phase]` to track progress
- [ ] Update this report as evidence is gathered
- [ ] Review checklist and mark completed items
- [ ] Sprint planning includes Service Standard prep tasks

**Fortnightly**:
- [ ] Team review of assessment readiness
- [ ] Practice show and tell with colleagues
- [ ] Gather feedback on presentation approach

**Before Booking**:
- [ ] All critical actions complete
- [ ] At least 10/14 points rated Green or Amber
- [ ] Team confident and prepared
- [ ] Documentation ready to share
- [ ] Demo environment tested and working

---

## Resources

### GDS Service Standard Resources

**Official Guidance**:
- [Service Standard](https://www.gov.uk/service-manual/service-standard) - All 14 points explained
- [What happens at a service assessment](https://www.gov.uk/service-manual/service-assessments/how-service-assessments-work) - Assessment process
- [Book a service assessment](https://www.gov.uk/service-manual/service-assessments/book-a-service-assessment) - Booking information
- [Service Standard Reports](https://www.gov.uk/service-standard-reports) - Browse 450+ published assessment reports

**Phase-Specific Guidance**:
- [Alpha phase](https://www.gov.uk/service-manual/agile-delivery/how-the-alpha-phase-works) - What to do in alpha
- [Beta phase](https://www.gov.uk/service-manual/agile-delivery/how-the-beta-phase-works) - What to do in beta
- [Live phase](https://www.gov.uk/service-manual/agile-delivery/how-the-live-phase-works) - What to do when live

**Deep Dives by Service Standard Point**:
[Links to all 14 individual point pages on GOV.UK]

### Related ArcKit Commands

**Complementary Analysis**:
- `/arckit.analyze` - Comprehensive governance quality analysis
- `/arckit.traceability` - Requirements traceability matrix showing evidence chains

**Overlap with TCoP**:
- `/arckit.tcop` - Technology Code of Practice assessment (points 11, 13 overlap)

**Generate Missing Evidence**:
- `/arckit.requirements` - If user stories or NFRs weak
- `/arckit.hld-review` - If architecture decisions not documented
- `/arckit.secure` - If security assessment incomplete
- `/arckit.diagram` - If architecture diagrams missing
- `/arckit.wardley` - If technology strategy not clear

### Community Resources

**Blog Posts and Lessons Learned**:
- [Preparing for a GDS assessment](https://www.iterate.org.uk/10-things-to-remember-when-preparing-for-a-service-standard-assessment/)
- [What I learned as a user researcher](https://dwpdigital.blog.gov.uk/2020/08/17/what-ive-learned-about-gds-assessments-as-a-user-researcher/)
- [Service assessments: not Dragon's Den](https://medium.com/deloitte-uk-design-blog/service-assessments-no-longer-dragons-den-909b56c43593)

**Supplier Support** (G-Cloud):
- Search Digital Marketplace for \"GDS assessment preparation\" support services
- Many suppliers offer assessment prep workshops and mock assessments

---

## Appendix: Assessment Outcome Examples

### Example: Strong Alpha Pass (Green)

**Typical characteristics**:
- 12-14 points rated Green
- Excellent user research with diverse participants
- Working prototype tested extensively
- Clear technology choices with justification
- Strong multidisciplinary team
- Agile practices established and working well

**Panel feedback themes**:
- \"Strong user research foundation\"
- \"Clear evidence of iteration based on feedback\"
- \"Team has right skills and working well together\"
- \"Technology choices well justified\"

### Example: Alpha with Amber

**Typical characteristics**:
- 8-11 points Green, 3-5 Amber, 0-1 Red
- Good user research but gaps in diversity
- Prototype exists but limited testing
- Technology chosen but not fully tested
- Team in place but some skills gaps

**Common amber points**:
- Point 1: Need more diverse user research participants
- Point 5: Accessibility considerations identified but not tested
- Point 8: Iterations happening but not clearly documented
- Point 12: Open source approach decided but not yet implemented

**Panel feedback themes**:
- \"Good start, needs more evidence of [X]\"
- \"Continue to build on [strength] and address [gap]\"
- \"By beta, we expect to see [specific improvement]\"

### Example: Beta with Critical Issues (Red)

**Typical characteristics**:
- Major gaps in 2-3 points
- Often accessibility (Point 5) or performance data (Point 10)
- Service working but quality issues
- Security or privacy concerns

**Common red points**:
- Point 5: WCAG 2.1 AA testing not completed (critical for beta)
- Point 9: Security testing not done or serious vulnerabilities found
- Point 10: No performance data being collected
- Point 14: Service unreliable, frequent downtime

**Panel feedback themes**:
- \"Cannot proceed to public beta until [critical issue] resolved\"
- \"This is essential for a beta service\"
- \"Team needs to prioritise [issue] immediately\"

---

**Report Generated by**: ArcKit v{ARC_VERSION} `/arckit.service-assessment` command

**Next Actions**:
1. Review this report with your team
2. Prioritize critical actions in your sprint planning
3. Re-run `/arckit.service-assessment PHASE=[phase]` weekly to track progress
4. Use checklist to track completion of preparation tasks

**Questions or Feedback**:
- Report issues: https://github.com/tractorjuice/arc-kit/issues
- Contribute improvements: PRs welcome
- Share your assessment experience: Help improve this command for others

---

*Good luck with your assessment! Remember: assessments are conversations about your service, not exams. Show your work, explain your thinking, and be open to feedback. The panel wants you to succeed.* üöÄ
```

## Operating Constraints

**Tone and Approach**:
- Supportive and constructive - you want the team to succeed
- Specific and actionable - avoid vague recommendations
- Realistic - don't overwhelm with too many actions
- Evidence-based - always reference specific artifacts and line numbers
- Phase-appropriate - adjust expectations based on alpha/beta/live

**Quality Standards**:
- Every gap must have a specific recommendation
- Every recommendation must have an owner, timeline, and outcome
- RAG ratings must be justified with evidence (or lack thereof)
- Assessment day guidance must be practical and specific
- Report must be comprehensive but scannable

**Important Notes**:
- This is a **preparation tool**, not the actual assessment
- Panel will make final decisions based on their expert judgment
- This command helps teams gather evidence and present it effectively
- Re-run weekly to track progress as evidence is gathered
- Assessment outcomes can't be guaranteed, but preparation increases success rate significantly

## Example Usage

```
/arckit.service-assessment PHASE=alpha DATE=2025-12-15
```

Generates: `projects/001-nhs-appointment/ARC-001-SVCASS-v1.0.md`

```
/arckit.service-assessment PHASE=beta
```

Generates: `projects/002-payment-gateway/ARC-002-SVCASS-v1.0.md`

## Success Indicators

**This command succeeds when**:
- Team feels confident and prepared for assessment
- All 14 Service Standard points have clear evidence or action plans
- Critical gaps identified and addressed before booking
- Team can present their work clearly on assessment day
- Assessment preparation time reduced from weeks to days
- Higher pass rates for teams using this tool

---

*Transform ArcKit documentation into Service Standard compliance evidence. Demonstrate governance excellence.* ‚ú®
"""
